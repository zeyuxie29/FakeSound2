# :mag_right: FakeSound2：A Benchmark for Explainable and Generalizable Deepfake Sound Detection
[![arXiv](https://img.shields.io/badge/arXiv-2509.17162-brightgreen.svg?style=flat-square)](https://arxiv.org/abs/2509.17162)
[![githubio](https://img.shields.io/badge/GitHub.io-Audio_Samples-blue?logo=Github&style=flat-square)](https://zeyuxie29.github.io/FakeSound2)
[![Hugging Face data](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/ZeyuXie/FakeSound2)

### Table of Contents

 - [Introduction](#introduction)
 - [Evaluation](#evaluation)
     - [Data Preparation](#preparation)
     - [Score Calculation](#metric)
 - [Training](#training)

***

### :hourglass: FakeSound：Deepfake General Audio Detection 

[![arXiv](https://img.shields.io/badge/arXiv-2406.08052-brightgreen.svg?style=flat-square)](https://arxiv.org/abs/2406.08052)
[![githubio](https://img.shields.io/badge/GitHub.io-Audio_Samples-blue?logo=Github&style=flat-square)](https://fakesounddata.github.io/)
[![Hugging Face data](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/ZeyuXie/FakeSound/tree/main)

<a id="introduction"></a>
### :collision: FakeSound2
We present **FakeSound2**, a benchmark designed to advance deepfake sound detection beyond binary accuracy and evaluate models across three dimensions: 
localization, traceability, and generalization, covering **6** manipulation types and **12** diverse sources:
* Localization: localize the temporal positions of forgery
* Traceability: identify the manipulation method and trace the accountable sources
* Generalization: generalizing to unseen sources

### :bulb: Limitations in Current Models
Through extensive experimentation with FakeSound2, we demonstrate that although current detection systems achieve high classification accuracy, they exhibit significant **limitations** in recognizing nuanced forged pattern distributions and providing reliable explanatory capabilities.

### :seedling: Inspiration
 Our findings highlight **two pivotal research directions** for advancing the field: 
* First, developing models capable of capturing fine-grained distributional discrepancies that arise from similar manipulation techniques or homogenized architectural patterns, thereby improving interpretability.
* Second, enabling models to characterize the authentic audio data distribution fundamentally rather than memorizing artificial forgery patterns, consequently enhancing generalization performance to unseen manipulations and sources.

***

<a id="evaluation"></a>

## Evaluation

<a id="preparation"></a>

### Data Preparation
For copyright reasons, please download the raw audio from the official [AudioCaps](https://audiocaps.github.io/).

The FakeSound2 benchmark can be downloaded from [HuggingfaceDataset](https://huggingface.co/datasets/ZeyuXie/FakeSound2).

The metadata is contained within *"meta/{}.json", with each line representing an individual sample in a dictionary format:
```python
{
    "fake_type": "Inpainting" # Type of manipulation (e.g., inpainting, editing, ...)
    "model": "AudioLDM2" # Source of the manipulated audio (e.g., AudioLDM2, Tango2, ...)
    "audio_id": "xxxxxxxxx" # file ID
    "onset_offset": "2-3_5.5-6" # Temporal timestamps of the forged segments
    "filepath": "data/test/audio/inpainting_ldm2/xxxxxxxxx.wav"# Filepath to the audio file  
    "target_caption": "a man talks, a dog barks"# Target caption generated by LLMs or rule-based scripts
    "add_event"/"splice_event": "a dog barks"# Selected event to be inserted (Optional, in addition, splice, editing)
    "delete_event": "a sprayer sprays"# Deleted event in original caption (Optional, in separation, addition, splice, editing)
}
```

### Install Dependencies

```shell
git clone https://github.com/zeyuxie29/FakeSound2
cd src
conda install --yes --file requirements.txt
```


<a id="metric"></a>
### Metric Calculation



We provide tools for running inference with audio manipulation detection models and evaluating their performance. The model outputs are saved in JSONL format：
```python
{
  "audio_id": "Q0anPAIkfBE_0.781-7.282", 
  "pred_binary": "fake", 
  "target_binary": "fake", 
  "pred_manipulation": "inpainting", 
  "target_manipulation": "inpainting", 
  "pred_source": "ldm2", 
  "target_source": "ldm2", 
  "pred_f1segment": "0.7-7.3", 
  "target_f1segment": "0.781-7.282"
}
```

​​Execute *inference.py* to generate predictions and subsequently calculate the performance metrics, or alternatively, run *evaluation.py* to compute the metrics.​
You may refer to *inference.py* for guidance on saving your model predictions and performing the corresponding computations.
```shell
  python inference.py --output_file_name {your predicted results}
  # or 
  # Use the --per_audio argument to save the prediction results for each audio file.​
  python inference.py --output_file_name {your predicted results} --per_audio
  python evluation.py --result_path {your predicted results}_perAudio
  
```

***

<a id="training"></a>
## Training
### Train & Inference

Install pre-trained models *EAT* into the *model/* directory.
```shell
cd models
mkdir EAT
cd EAT
git clone https://github.com/pytorch/fairseq
cd fairseq
pip install --editable ./
cd ..
git clone https://github.com/cwx-worst-one/EAT
```

The training and testing codes are named *train.py* and *inference.py*, respectively.
```shell
  python train.py
  python inference.py
```


## Acknowledgement
Our code referred to the [DKU speech deepfake detection](https://github.com/caizexin/speechbrain_PartialFake), [EAT](https://github.com/cwx-worst-one/EAT). We appreciate their open-sourcing of their code.

